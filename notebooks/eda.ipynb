{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch_html(url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content from the given URL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL of the webpage to fetch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The HTML content of the webpage. Returns None if the request fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_table(html, headers, columns_to_clean=None):\n",
    "    \"\"\"\n",
    "    General function to parse HTML table content and return it as a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html : str\n",
    "        The HTML content of the webpage.\n",
    "    headers : list\n",
    "        List of column headers for the table.\n",
    "    columns_to_clean : dict, optional\n",
    "        Dictionary where the key is the column name and the value is a function to clean the column (default is None).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the table data. Returns an empty DataFrame if no table is found.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"class\": \"standard_tabelle\"})\n",
    "\n",
    "    if not table:\n",
    "        print(\"Table not found on the page.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract table rows\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    # Prepare data for the DataFrame\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        processed_columns = [col.text.strip().split(\"\\n\")[-1] for col in columns]\n",
    "        data.append(processed_columns)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "    # Clean specific columns if needed\n",
    "    if columns_to_clean:\n",
    "        for col, cleaning_function in columns_to_clean.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(cleaning_function)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_goals_column(goal_str):\n",
    "    \"\"\"\n",
    "    Cleans the 'Goals' column by extracting the number of goals, ignoring penalty information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    goal_str : str\n",
    "        The raw goals data in the format 'X (Y penalties)'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The cleaned goal number.\n",
    "    \"\"\"\n",
    "    return goal_str.split(\" \")[0]\n",
    "\n",
    "\n",
    "def parse_goals_table(html):\n",
    "    \"\"\"\n",
    "    Parses the HTML content and extracts the goals table data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html : str\n",
    "        The HTML content of the webpage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the goal data.\n",
    "    \"\"\"\n",
    "    headers = [\"#\", \"Player\", \"\", \"Country\", \"Team\", \"Goals\"]\n",
    "    columns_to_clean = {\"Goals\": clean_goals_column}\n",
    "    return parse_table(html=html, headers=headers, columns_to_clean=columns_to_clean)\n",
    "\n",
    "\n",
    "def parse_assists_table(html):\n",
    "    \"\"\"\n",
    "    Parses the HTML content and extracts the assists table data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html : str\n",
    "        The HTML content of the webpage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the assist data.\n",
    "    \"\"\"\n",
    "    headers = [\"#\", \"Player\", \"\", \"Country\", \"Team\", \"Assists\"]\n",
    "    return parse_table(html=html, headers=headers)\n",
    "\n",
    "\n",
    "def get_season_data(url, season, parse_function, columns_to_clean=None, sleep_time=0.5):\n",
    "    \"\"\"\n",
    "    Gets data for a specific season from the given URL using a specified parsing function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL of the webpage to get data from.\n",
    "    season : str\n",
    "        The season for which to get the data.\n",
    "    parse_function : function\n",
    "        A function that takes the HTML content and returns a DataFrame.\n",
    "    columns_to_clean : dict, optional\n",
    "        A dictionary where the key is a column name and the value is a function to clean the column (default is None).\n",
    "    sleep_time : float, optional\n",
    "        Time to sleep between requests to avoid overloading the server (default is 0.5 seconds).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the data for the given season. Returns an empty DataFrame if getting data fails.\n",
    "    \"\"\"\n",
    "    html = fetch_html(url=url)\n",
    "    if html is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Use the provided parsing function to extract data\n",
    "    df = parse_function(html)\n",
    "\n",
    "    # Clean specific columns if needed\n",
    "    if columns_to_clean:\n",
    "        for col, cleaning_function in columns_to_clean.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(cleaning_function)\n",
    "\n",
    "    # Add season column\n",
    "    if not df.empty:\n",
    "        df[\"Season\"] = season\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[\"#\", \"\"], errors=\"ignore\")\n",
    "\n",
    "    # Sleep to avoid overloading the server\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_all_season_data(seasons, parse_function, columns_to_clean=None, sleep_time=0.5):\n",
    "    \"\"\"\n",
    "    Gets data for multiple seasons and writes them as individual CSVs using a specified parsing function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seasons : dict\n",
    "        A dictionary where keys are season strings and values are URLs to get data from for those seasons.\n",
    "    parse_function : function\n",
    "        A function that takes the HTML content and returns a DataFrame.\n",
    "    columns_to_clean : dict, optional\n",
    "        A dictionary where the key is a column name and the value is a function to clean the column (default is None).\n",
    "    sleep_time : float, optional\n",
    "        Time to sleep between requests to avoid overloading the server (default is 0.5 seconds).\n",
    "    \"\"\"\n",
    "    # Ensure the data directory exists\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    for season, url in seasons.items():\n",
    "        print(f\"Getting data for season {season}...\")\n",
    "\n",
    "        # Fetch data for the current season\n",
    "        season_data = get_season_data(\n",
    "            url=url,\n",
    "            season=season,\n",
    "            parse_function=parse_function,\n",
    "            columns_to_clean=columns_to_clean,\n",
    "            sleep_time=sleep_time\n",
    "        )\n",
    "\n",
    "        if not season_data.empty:\n",
    "            # Define file path\n",
    "            file_path = f\"data/championship_{season}.csv\"\n",
    "\n",
    "            # Save each season's data to a separate CSV file\n",
    "            season_data.to_csv(file_path, index=False)\n",
    "            print(f\"Data for season {season} saved to {file_path}.\")\n",
    "        else:\n",
    "            print(f\"No data available for season {season}.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
